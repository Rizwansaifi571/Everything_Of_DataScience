{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96709ae5",
   "metadata": {},
   "source": [
    "# ğŸ¯ Natural Language Processing: Lemmatization Mastery\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What is Lemmatization?\n",
    "\n",
    "**Lemmatization** is an advanced text normalization technique that transforms words into their **dictionary base form** (called **lemma**) using vocabulary and morphological analysis. Unlike stemming, lemmatization always produces real, valid words.\n",
    "\n",
    "### ğŸ“– Simple Definition:\n",
    "> *\"Lemmatization is the intelligent way of reducing words to their meaningful base form using linguistic knowledge.\"*\n",
    "\n",
    "### ğŸ” Key Characteristics:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Output** | Always a valid dictionary word |\n",
    "| **Method** | Uses vocabulary + morphological analysis |\n",
    "| **Accuracy** | âœ… High - understands context and meaning |\n",
    "| **Speed** | ğŸ¢ Slower than stemming (requires dictionary lookup) |\n",
    "| **Knowledge Base** | Uses WordNet lexical database |\n",
    "| **Context-Aware** | âœ… Yes - considers part of speech |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Examples: The Power of Lemmatization\n",
    "\n",
    "### Example 1: Verb Forms\n",
    "```\n",
    "Original:  running, runs, ran, runner\n",
    "Stemming:  run, run, ran, runner  âŒ (inconsistent)\n",
    "Lemmatization: run, run, run, run  âœ… (all to base form)\n",
    "```\n",
    "\n",
    "### Example 2: Adjectives\n",
    "```\n",
    "Original:  better, best, good\n",
    "Stemming:  better, best, good  âŒ (no reduction)\n",
    "Lemmatization: good, good, good  âœ… (semantic root!)\n",
    "```\n",
    "\n",
    "### Example 3: Context Matters\n",
    "```\n",
    "Word: \"saw\"\n",
    "\n",
    "As a noun (tool):     saw â†’ saw\n",
    "As a verb (past):     saw â†’ see  âœ… (context-aware!)\n",
    "```\n",
    "\n",
    "### Example 4: Complex Forms\n",
    "```\n",
    "Original:  studies, studying, studied\n",
    "Stemming:  studi, studi, studi  âŒ (not real words)\n",
    "Lemmatization: study, study, study  âœ… (perfect!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Why is Lemmatization Important?\n",
    "\n",
    "### 1. ğŸ“– **Produces Real Words**\n",
    "- Output is always found in dictionary\n",
    "- Human-readable results\n",
    "- Better for customer-facing applications\n",
    "\n",
    "### 2. ğŸ§  **Semantic Understanding**\n",
    "- Understands word meaning and context\n",
    "- \"better\" â†’ \"good\" (knows they're related!)\n",
    "- More intelligent than simple rule-based stemming\n",
    "\n",
    "### 3. ğŸ¤ **Better for Text Generation**\n",
    "- Chatbots and conversational AI\n",
    "- Text summarization\n",
    "- Machine translation\n",
    "- Any application where output needs to be readable\n",
    "\n",
    "### 4. ğŸ” **Improved Accuracy**\n",
    "- More precise in information retrieval\n",
    "- Better feature extraction for ML\n",
    "- Reduced false positives in search\n",
    "\n",
    "### 5. ğŸŒ **Language Awareness**\n",
    "- Handles irregular verbs (go â†’ went â†’ gone)\n",
    "- Understands plurals, tenses, comparatives\n",
    "- Language-specific rules applied\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†š Lemmatization vs Stemming: The Ultimate Comparison\n",
    "\n",
    "### Quick Comparison Table\n",
    "\n",
    "| Aspect | Stemming | Lemmatization |\n",
    "|--------|----------|---------------|\n",
    "| **Output** | May be non-word (e.g., \"studi\") | Always real word (e.g., \"study\") |\n",
    "| **Method** | Rule-based chopping | Dictionary + morphology |\n",
    "| **Speed** | âš¡âš¡âš¡ Very Fast | ğŸ¢ Slower (dictionary lookup) |\n",
    "| **Accuracy** | â­â­â­ Good | â­â­â­â­â­ Excellent |\n",
    "| **Context** | âŒ No context awareness | âœ… Uses POS tags |\n",
    "| **Example 1** | \"caring\" â†’ \"car\" âŒ | \"caring\" â†’ \"care\" âœ… |\n",
    "| **Example 2** | \"better\" â†’ \"better\" | \"better\" â†’ \"good\" âœ… |\n",
    "| **Use Case** | Search, IR, speed-critical | Chatbots, semantic tasks |\n",
    "| **Resource** | Minimal | Needs WordNet database |\n",
    "\n",
    "### ğŸ”¬ Visual Comparison\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STEMMING                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Input: \"studies, studying, studied\"                    â”‚\n",
    "â”‚  Process: Apply rules â†’ Remove suffixes                 â”‚\n",
    "â”‚  Output: \"studi, studi, studi\"  âŒ Not real words      â”‚\n",
    "â”‚  Speed: âš¡âš¡âš¡ 45,000 words/sec                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  LEMMATIZATION                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Input: \"studies, studying, studied\"                    â”‚\n",
    "â”‚  Process: Dictionary lookup + POS analysis              â”‚\n",
    "â”‚  Output: \"study, study, study\"  âœ… Real words          â”‚\n",
    "â”‚  Speed: ğŸ¢ ~5,000 words/sec (slower but smarter)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“Š When to Use Each\n",
    "\n",
    "```\n",
    "Use STEMMING when:\n",
    "â”œâ”€â”€ âš¡ Speed is critical\n",
    "â”œâ”€â”€ ğŸ“Š Working with large corpora (millions of documents)\n",
    "â”œâ”€â”€ ğŸ” Information retrieval (search engines)\n",
    "â”œâ”€â”€ ğŸ“ˆ Document clustering\n",
    "â””â”€â”€ ğŸ’¾ Memory constraints\n",
    "\n",
    "Use LEMMATIZATION when:\n",
    "â”œâ”€â”€ ğŸ¤ Building chatbots or conversational AI\n",
    "â”œâ”€â”€ ğŸ“ Text summarization\n",
    "â”œâ”€â”€ ğŸ˜Š Sentiment analysis (nuance matters)\n",
    "â”œâ”€â”€ ğŸŒ Machine translation\n",
    "â”œâ”€â”€ ğŸ“– Text generation\n",
    "â””â”€â”€ ğŸ¯ Accuracy > Speed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—„ï¸ Understanding WordNet\n",
    "\n",
    "### What is WordNet?\n",
    "\n",
    "**WordNet** is a large **lexical database** of English developed at Princeton University. It's the backbone of NLTK's lemmatization.\n",
    "\n",
    "### ğŸ”‘ Key Features:\n",
    "\n",
    "1. **Synsets (Synonym Sets)**\n",
    "   - Groups of words with similar meanings\n",
    "   - Example: {car, auto, automobile, machine}\n",
    "\n",
    "2. **Semantic Relationships**\n",
    "   - Hypernyms (more general): vehicle â†’ car\n",
    "   - Hyponyms (more specific): car â†’ sedan\n",
    "   - Antonyms (opposites): good â†” bad\n",
    "\n",
    "3. **Parts of Speech**\n",
    "   - Nouns, Verbs, Adjectives, Adverbs\n",
    "   - Each with specific lemmatization rules\n",
    "\n",
    "4. **Coverage**\n",
    "   - Over 155,000 words\n",
    "   - 117,000+ synonym sets\n",
    "   - Constantly updated\n",
    "\n",
    "### ğŸ¯ Why WordNet Matters for Lemmatization:\n",
    "\n",
    "```python\n",
    "# WordNet helps lemmatizer understand:\n",
    "\"better\" â†’ knows this is comparative of \"good\"\n",
    "\"went\" â†’ knows this is past tense of \"go\"\n",
    "\"mice\" â†’ knows this is plural of \"mouse\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ·ï¸ Understanding Part-of-Speech (POS) Tags\n",
    "\n",
    "### What are POS Tags?\n",
    "\n",
    "**POS tags** tell the lemmatizer what type of word it's dealing with. This is **CRITICAL** because the same word can have different lemmas depending on its role in a sentence.\n",
    "\n",
    "### ğŸ“‹ NLTK POS Tags for Lemmatization\n",
    "\n",
    "| POS Tag | Meaning | Examples | Usage |\n",
    "|---------|---------|----------|-------|\n",
    "| **'n'** | Noun | book, cat, history | Things, people, places |\n",
    "| **'v'** | Verb | run, eat, write | Actions, states |\n",
    "| **'a'** | Adjective | happy, big, blue | Describes nouns |\n",
    "| **'r'** | Adverb | quickly, very, fairly | Describes verbs/adjectives |\n",
    "\n",
    "### ğŸ” Why POS Tags Matter\n",
    "\n",
    "**Example: The word \"saw\"**\n",
    "\n",
    "```python\n",
    "# Without POS (defaults to noun):\n",
    "lemmatize(\"saw\")  â†’ \"saw\" (the tool)\n",
    "\n",
    "# With POS as verb:\n",
    "lemmatize(\"saw\", pos='v')  â†’ \"see\" (past tense of see) âœ…\n",
    "\n",
    "# With POS as noun:\n",
    "lemmatize(\"saw\", pos='n')  â†’ \"saw\" (the cutting tool) âœ…\n",
    "```\n",
    "\n",
    "### âš ï¸ Common Mistakes\n",
    "\n",
    "```python\n",
    "# âŒ WRONG - Using wrong POS tag\n",
    "lemmatize(\"running\", pos='n')  â†’ \"running\" (no change)\n",
    "\n",
    "# âœ… CORRECT - Using correct POS tag\n",
    "lemmatize(\"running\", pos='v')  â†’ \"run\" (base form of verb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Getting Started\n",
    "\n",
    "Before we begin, ensure you have:\n",
    "- âœ… NLTK installed\n",
    "- âœ… WordNet corpus downloaded\n",
    "- âœ… Understanding of POS tags\n",
    "\n",
    "### ğŸ“¦ Quick Setup:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”§ Part 1: Understanding POS Tags in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9b435",
   "metadata": {},
   "source": [
    "## ğŸ¯ Demonstration: The Importance of Correct POS Tags\n",
    "\n",
    "Let's see how using the **wrong** vs **right** POS tag dramatically affects the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c772828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n",
      "goes\n",
      "go\n",
      "eaten\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"going\", pos=\"n\")) ## wrong part of speech\n",
    "print(lemmatizer.lemmatize(\"going\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"goes\", pos = \"a\")) ## wrong part of speech\n",
    "print(lemmatizer.lemmatize(\"goes\", pos = \"v\"))\n",
    "print(lemmatizer.lemmatize(\"eaten\", pos='r'))   ## wrong part of speech\n",
    "print(lemmatizer.lemmatize(\"eaten\", pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a597d67",
   "metadata": {},
   "source": [
    "### ğŸ” Analysis of Results\n",
    "\n",
    "Let's break down what happened in the above code:\n",
    "\n",
    "#### Test 1: \"going\"\n",
    "```python\n",
    "lemmatize(\"going\", pos=\"n\")  # Wrong: treating verb as noun\n",
    "â†’ \"going\" (no change) âŒ\n",
    "\n",
    "lemmatize(\"going\", pos=\"v\")  # Correct: verb\n",
    "â†’ \"go\" (correct base form) âœ…\n",
    "```\n",
    "\n",
    "#### Test 2: \"goes\"\n",
    "```python\n",
    "lemmatize(\"goes\", pos=\"a\")  # Wrong: treating verb as adjective\n",
    "â†’ \"goes\" (no change) âŒ\n",
    "\n",
    "lemmatize(\"goes\", pos=\"v\")  # Correct: verb\n",
    "â†’ \"go\" (correct base form) âœ…\n",
    "```\n",
    "\n",
    "#### Test 3: \"eaten\"\n",
    "```python\n",
    "lemmatize(\"eaten\", pos='r')  # Wrong: treating verb as adverb\n",
    "â†’ \"eaten\" (no change) âŒ\n",
    "\n",
    "lemmatize(\"eaten\", pos='v')  # Correct: verb (past participle)\n",
    "â†’ \"eat\" (correct base form) âœ…\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Key Insight:\n",
    "\n",
    "> **POS tags are CRITICAL for accurate lemmatization!**\n",
    "> \n",
    "> Using the wrong POS tag will result in no transformation, defeating the purpose of lemmatization.\n",
    "\n",
    "### ğŸ“Š POS Tag Quick Reference:\n",
    "\n",
    "| If the word is... | Use POS | Example |\n",
    "|------------------|---------|---------|\n",
    "| An action (run, eat, write) | `pos='v'` | eating â†’ eat |\n",
    "| A thing (book, cat, idea) | `pos='n'` | mice â†’ mouse |\n",
    "| Descriptive (happy, big) | `pos='a'` | better â†’ good |\n",
    "| Modifies verb (quickly, very) | `pos='r'` | finally â†’ finally |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Preparing Test Data\n",
    "\n",
    "Let's create a comprehensive test set to see lemmatization in action across different word types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f7fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\", \"congratulation\", \"ingeating\", \"fairly\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf5134",
   "metadata": {},
   "source": [
    "### ğŸ“Š Test Word Analysis\n",
    "\n",
    "Our test set includes diverse word forms:\n",
    "\n",
    "| Word | Type | Expected Lemma (as verb) |\n",
    "|------|------|--------------------------|\n",
    "| eating | Verb (gerund) | eat |\n",
    "| eats | Verb (3rd person) | eat |\n",
    "| eaten | Verb (past participle) | eat |\n",
    "| writing | Verb (gerund) | write |\n",
    "| writes | Verb (3rd person) | write |\n",
    "| programming | Verb (gerund) | program |\n",
    "| programs | Verb/Noun | program |\n",
    "| history | Noun | history |\n",
    "| finally | Adverb | finally |\n",
    "| finalized | Verb (past) | finalize |\n",
    "| congratulation | Noun | congratulation |\n",
    "| ingeating | Invalid word | ingeating |\n",
    "| fairly | Adverb | fairly |\n",
    "| sportingly | Adverb | sportingly |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”§ Part 2: Batch Lemmatization with Verbs\n",
    "\n",
    "Now let's lemmatize all words treating them as **verbs** (pos='v'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39cfa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -----> eat\n",
      "eats -----> eat\n",
      "eaten -----> eat\n",
      "writing -----> write\n",
      "writes -----> write\n",
      "programming -----> program\n",
      "programs -----> program\n",
      "history -----> history\n",
      "finally -----> finally\n",
      "finalized -----> finalize\n",
      "congratulation -----> congratulation\n",
      "ingeating -----> ingeating\n",
      "fairly -----> fairly\n",
      "sportingly -----> sportingly\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"----->\", lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c4a3f",
   "metadata": {},
   "source": [
    "### ğŸ“Š Expected Comparison Results\n",
    "\n",
    "| Word | Porter Stem | Snowball Stem | Lemmatization (noun) | Winner |\n",
    "|------|-------------|---------------|----------------------|--------|\n",
    "| **better** | better | better | better | âš ï¸ None (need adj POS) |\n",
    "| **caring** | car âŒ | care | caring | ğŸ† Snowball/Lemma |\n",
    "| **studies** | studi âŒ | studi âŒ | study âœ… | ğŸ† Lemmatization |\n",
    "| **feet** | feet | feet | foot âœ… | ğŸ† Lemmatization |\n",
    "| **geese** | gees | gees | goose âœ… | ğŸ† Lemmatization |\n",
    "| **teeth** | teeth | teeth | tooth âœ… | ğŸ† Lemmatization |\n",
    "| **mice** | mice | mice | mouse âœ… | ğŸ† Lemmatization |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
