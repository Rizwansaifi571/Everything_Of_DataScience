{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f938334",
   "metadata": {},
   "source": [
    "# üî§ Natural Language Processing: Tokenization Fundamentals\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. [Introduction to Tokenization](#introduction)\n",
    "2. [Why Tokenization Matters](#why-it-matters)\n",
    "3. [Types of Tokenization](#types)\n",
    "4. [Practical Implementation](#implementation)\n",
    "5. [Comparison of Methods](#comparison)\n",
    "6. [Best Practices](#best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is Tokenization?\n",
    "\n",
    "**Tokenization** is the fundamental first step in Natural Language Processing (NLP) that involves breaking down text into smaller, meaningful units called **tokens**. These tokens can be:\n",
    "\n",
    "- **Sentences** (from paragraphs)\n",
    "- **Words** (from sentences)\n",
    "- **Characters** (from words)\n",
    "- **Subwords** (for advanced models)\n",
    "\n",
    "### üîç Why is Tokenization Important?\n",
    "\n",
    "> *\"Before a computer can understand language, it must first break it down into digestible pieces.\"*\n",
    "\n",
    "Tokenization is crucial because:\n",
    "\n",
    "1. üß† **Computer Understanding**: Machines process discrete units, not continuous text\n",
    "2. üìä **Feature Extraction**: Tokens become features for ML models\n",
    "3. üîç **Text Analysis**: Enables counting, filtering, and pattern recognition\n",
    "4. üéØ **Preprocessing**: First step before stemming, lemmatization, or vectorization\n",
    "5. üìà **Statistical Analysis**: Allows frequency analysis and corpus statistics\n",
    "\n",
    "### üåü Real-World Applications\n",
    "\n",
    "- **Search Engines**: Breaking queries into searchable terms\n",
    "- **Chatbots**: Understanding user messages\n",
    "- **Sentiment Analysis**: Analyzing product reviews\n",
    "- **Machine Translation**: Google Translate, DeepL\n",
    "- **Text Classification**: Spam detection, topic categorization\n",
    "- **Information Extraction**: Named Entity Recognition (NER)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Step 1: Installation and Setup\n",
    "\n",
    "Let's start by installing the Natural Language Toolkit (NLTK), the most popular Python library for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbf493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d63a2b",
   "metadata": {},
   "source": [
    "### üìù About NLTK\n",
    "\n",
    "**NLTK (Natural Language Toolkit)** is a leading platform for building Python programs to work with human language data. It provides:\n",
    "- Easy-to-use interfaces to over 50 corpora and lexical resources\n",
    "- Text processing libraries for classification, tokenization, stemming, tagging, parsing, and more\n",
    "- Wrappers for industrial-strength NLP libraries\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Step 2: Prepare Sample Text\n",
    "\n",
    "Let's create a sample corpus (text data) to demonstrate different tokenization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bf39d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello Welcome, to Rizwan's NLP Tutorials. \n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb01a7",
   "metadata": {},
   "source": [
    "### üìä Understanding the Corpus\n",
    "\n",
    "Our sample text contains:\n",
    "- ‚úÖ Multiple sentences\n",
    "- ‚úÖ Punctuation marks (commas, periods, exclamation marks)\n",
    "- ‚úÖ Possessive forms (Rizwan's)\n",
    "- ‚úÖ Contractions and special cases\n",
    "\n",
    "This diverse text will help us understand how different tokenizers handle various linguistic features.\n",
    "\n",
    "Let's visualize our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c4d9871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Rizwan's NLP Tutorials. \n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126290e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Step 3: Download NLTK Data\n",
    "\n",
    "NLTK requires additional data files for tokenization. The `punkt` tokenizer is a pre-trained model that knows where to split sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08ca26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5934b",
   "metadata": {},
   "source": [
    "### üîç What is Punkt?\n",
    "\n",
    "**Punkt** is an unsupervised trainable model for sentence tokenization that:\n",
    "- Detects sentence boundaries intelligently\n",
    "- Handles abbreviations (Dr., Mr., etc.)\n",
    "- Recognizes decimal numbers (3.14)\n",
    "- Understands common punctuation patterns\n",
    "\n",
    "---\n",
    "\n",
    "# üìñ Part 1: Sentence Tokenization\n",
    "\n",
    "## üéØ Definition: Sentence Tokenization\n",
    "\n",
    "**Sentence Tokenization** (also called **Sentence Segmentation**) is the process of dividing a text document into individual sentences.\n",
    "\n",
    "### How It Works:\n",
    "- Looks for sentence-ending punctuation (. ! ?)\n",
    "- Considers context (abbreviations, decimals)\n",
    "- Uses machine learning models trained on text corpora\n",
    "\n",
    "### Use Cases:\n",
    "- üì∞ Summarization: Extracting key sentences\n",
    "- üé§ Speech synthesis: Natural pausing\n",
    "- üìä Text analysis: Sentence-level metrics\n",
    "- üîç Information retrieval: Sentence-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57107699",
   "metadata": {},
   "source": [
    "## üîß Implementation: Sentence Tokenization\n",
    "\n",
    "Let's use `sent_tokenize()` to split our corpus into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd477bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome, to Rizwan's NLP Tutorials.\",\n",
       " 'Please do watch the entire course!',\n",
       " 'to become expert in NLP.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "document = sent_tokenize(corpus)\n",
    "\n",
    "document    # store all document in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fcd0d",
   "metadata": {},
   "source": [
    "### üìä Result Analysis\n",
    "\n",
    "Notice how `sent_tokenize()`:\n",
    "- ‚úÖ Correctly identifies 2 sentences\n",
    "- ‚úÖ Handles the comma in the first sentence (doesn't split there)\n",
    "- ‚úÖ Stores sentences in a Python list for easy processing\n",
    "- ‚úÖ Preserves original punctuation and spacing\n",
    "\n",
    "Let's print each sentence separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5a134fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Rizwan's NLP Tutorials.\n",
      "Please do watch the entire course!\n",
      "to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e17fe",
   "metadata": {},
   "source": [
    "### üéØ Key Observation\n",
    "\n",
    "Each sentence is now a separate element, making it easy to:\n",
    "- Analyze sentence length\n",
    "- Process sentences individually\n",
    "- Perform sentence-level operations (translation, sentiment analysis)\n",
    "\n",
    "---\n",
    "\n",
    "# üìñ Part 2: Word Tokenization\n",
    "\n",
    "## üéØ Definition: Word Tokenization\n",
    "\n",
    "**Word Tokenization** is the process of splitting text into individual words or tokens. This is the most common type of tokenization used in NLP.\n",
    "\n",
    "### Challenges in Word Tokenization:\n",
    "1. **Punctuation**: Should \"don't\" be one token or two?\n",
    "2. **Possessives**: How to handle \"Rizwan's\"?\n",
    "3. **Contractions**: \"can't\", \"won't\", \"I'm\"\n",
    "4. **Hyphenated words**: \"state-of-the-art\"\n",
    "5. **Special characters**: Emails, URLs, hashtags\n",
    "\n",
    "Different tokenizers handle these challenges differently!\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Method 1: Standard Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14bd9e",
   "metadata": {},
   "source": [
    "### üìù About word_tokenize()\n",
    "\n",
    "`word_tokenize()` is NLTK's default word tokenizer that:\n",
    "- Splits on whitespace and punctuation\n",
    "- **Keeps contractions together** (e.g., \"Rizwan's\" ‚Üí \"Rizwan\" + \"'s\")\n",
    "- Handles most common cases intelligently\n",
    "- Based on the TreebankWordTokenizer with additional improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7fa6aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Rizwan',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)   # punchuation not seperated example 's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e962bf6",
   "metadata": {},
   "source": [
    "### üîç Observation: word_tokenize() Behavior\n",
    "\n",
    "**Key Points:**\n",
    "- ‚úÖ Separates most punctuation from words\n",
    "- ‚úÖ Handles possessives: \"Rizwan's\" ‚Üí [\"Rizwan\", \"'s\"]\n",
    "- ‚úÖ Keeps exclamation marks separate: \"course\" and \"!\" are different tokens\n",
    "- ‚úÖ Commas are treated as separate tokens\n",
    "\n",
    "**When to Use:** General-purpose tokenization for most NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "Now let's tokenize each sentence individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1c66278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Rizwan', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f30f8",
   "metadata": {},
   "source": [
    "### üí° Practical Application\n",
    "\n",
    "This sentence-then-word tokenization approach is useful for:\n",
    "- **Document structure preservation**: Maintaining sentence boundaries\n",
    "- **Sentence-level analysis**: Calculating metrics per sentence\n",
    "- **Better context**: Keeping word relationships within sentences\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Method 2: Punctuation-Aware Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5fe2e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Rizwan',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    " \n",
    "wordpunct_tokenize(corpus)   ## not punchuation also treated as single word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38726a",
   "metadata": {},
   "source": [
    "### üìù About wordpunct_tokenize()\n",
    "\n",
    "`wordpunct_tokenize()` is more aggressive with punctuation:\n",
    "\n",
    "**Characteristics:**\n",
    "- üîç Splits on **ANY** punctuation character\n",
    "- üîç Treats each punctuation mark as a separate token\n",
    "- üîç \"Rizwan's\" ‚Üí [\"Rizwan\", \"'\", \"s\"] (3 tokens!)\n",
    "- üîç Even commas, apostrophes, periods become individual tokens\n",
    "\n",
    "**Advantage:** Maximum granularity - captures every character type\n",
    "**Disadvantage:** May over-split meaningful units like contractions\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Comparison: word_tokenize vs wordpunct_tokenize\n",
    "\n",
    "| Feature | word_tokenize | wordpunct_tokenize |\n",
    "|---------|---------------|-------------------|\n",
    "| **Possessives** | \"Rizwan's\" ‚Üí [\"Rizwan\", \"'s\"] | \"Rizwan's\" ‚Üí [\"Rizwan\", \"'\", \"s\"] |\n",
    "| **Contractions** | \"don't\" ‚Üí [\"do\", \"n't\"] | \"don't\" ‚Üí [\"don\", \"'\", \"t\"] |\n",
    "| **Punctuation** | Smart handling | Every punct is separate |\n",
    "| **Use Case** | General NLP | Character-level analysis |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Method 3: TreebankWord Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9212b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Rizwan',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(corpus)  ## except last line full stop not treated as single token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aec8ea",
   "metadata": {},
   "source": [
    "### üìù About TreebankWordTokenizer\n",
    "\n",
    "The **TreebankWordTokenizer** follows the Penn Treebank tokenization standard used in linguistic research.\n",
    "\n",
    "**Key Features:**\n",
    "- üìö Based on the Penn Treebank project (widely used linguistic corpus)\n",
    "- üìö Specific rules for contractions, possessives, and punctuation\n",
    "- üìö Standard in many NLP research papers\n",
    "- üìö More consistent with linguistic conventions\n",
    "\n",
    "**Behavior:**\n",
    "- Splits contractions: \"don't\" ‚Üí \"do\" + \"n't\"\n",
    "- Handles possessives: \"Rizwan's\" ‚Üí \"Rizwan\" + \"'s\"\n",
    "- Period handling: Separates sentence-final periods but handles abbreviations\n",
    "\n",
    "**When to Use:**\n",
    "- Academic research requiring standard tokenization\n",
    "- Comparing with published NLP papers\n",
    "- Need for linguistic consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241458a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Summary & Best Practices\n",
    "\n",
    "## üéØ Quick Reference Guide\n",
    "\n",
    "### When to Use Each Tokenizer:\n",
    "\n",
    "| Tokenizer | Best For | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **sent_tokenize** | Sentence segmentation | Accurate, handles abbreviations | Only for sentences |\n",
    "| **word_tokenize** | General NLP tasks | Balanced, smart handling | May not fit all use cases |\n",
    "| **wordpunct_tokenize** | Character analysis | Maximum granularity | Over-splits contractions |\n",
    "| **TreebankWordTokenizer** | Research, consistency | Standard, reproducible | More complex rules |\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Best Practices for Tokenization\n",
    "\n",
    "### 1. **Choose Based on Your Task**\n",
    "```\n",
    "Text Classification ‚Üí word_tokenize (balanced approach)\n",
    "Sentiment Analysis ‚Üí word_tokenize or TreebankWordTokenizer\n",
    "Character-level NLP ‚Üí wordpunct_tokenize\n",
    "Research/Papers ‚Üí TreebankWordTokenizer (reproducibility)\n",
    "```\n",
    "\n",
    "### 2. **Consider Language and Domain**\n",
    "- Different languages need different tokenizers\n",
    "- Social media text may need specialized tokenizers (handling emojis, hashtags)\n",
    "- Technical documents may need custom rules\n",
    "\n",
    "### 3. **Preprocessing Pipeline**\n",
    "Typical NLP pipeline:\n",
    "```\n",
    "1. Sentence Tokenization (sent_tokenize)\n",
    "2. Word Tokenization (word_tokenize)\n",
    "3. Lowercasing\n",
    "4. Remove stopwords\n",
    "5. Stemming/Lemmatization\n",
    "6. Vectorization\n",
    "```\n",
    "\n",
    "### 4. **Handle Edge Cases**\n",
    "- **URLs**: May need special handling\n",
    "- **Emails**: Should be kept together\n",
    "- **Numbers**: Consider keeping decimal points\n",
    "- **Hashtags**: Important for social media analysis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
